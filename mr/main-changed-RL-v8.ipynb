{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85963213",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==================================================================================\n",
    "#       Copyright (c) 2020 China Mobile Technology (USA) Inc. Intellectual Property.\n",
    "#\n",
    "#   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#   you may not use this file except in compliance with the License.\n",
    "#   You may obtain a copy of the License at\n",
    "#\n",
    "#          http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#   Unless required by applicable law or agreed to in writing, software\n",
    "#   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#   See the License for the specific language governing permissions and\n",
    "#   limitations under the License.\n",
    "# ==================================================================================\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "#sys.path.append('C:/Users/Mohammadreza/Desktop/My Class/Proj-DC/My Works/Scheduling/xApp/mr7-main/mr9_github')\n",
    "sys.path.append('.')\n",
    "import schedule\n",
    "from zipfile import ZipFile\n",
    "import json\n",
    "from os import getenv\n",
    "from ricxappframe.xapp_frame import RMRXapp, rmr, Xapp\n",
    "from mr9_github import sdl\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from numpy import zeros, newaxis\n",
    "\n",
    "from mr.db import DATABASE, DUMMY\n",
    "import mr.populate as populate\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "from statistics import mean\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython\n",
    "from IPython import display\n",
    "\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import LSTM, Dense  \n",
    "from tensorflow.keras.layers import Activation  \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from mr import mobile_env\n",
    "from mobile_env.handlers.central import MComCentralHandler\n",
    "from mobile_env.core.base import MComCore\n",
    "from mobile_env.core.entities import BaseStation, UserEquipment\n",
    "from mobile_env.scenarios.small import MComSmall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23e27c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MComSmall.default_config()\n",
    "\n",
    "env = gym.make(\"mobile-small-central-v0\")\n",
    "\n",
    "num_states = 7\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_whole_states = 35\n",
    "print(\"Size of Whole State Space ->  {}\".format(num_whole_states))\n",
    "num_actions = 4\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "num_ues = 5\n",
    "upper_bound = env.NUM_STATIONS\n",
    "lower_bound = 0\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))\n",
    "# Configuration parameters for the whole setup\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "max_steps_per_episode = 50\n",
    "eps = np.finfo(np.float32).eps.item()  # Smallest number such that 1.0 + eps != 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd3e0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_inputs = 4\n",
    "# num_actions = 2\n",
    "num_hidden1 = 64\n",
    "num_hidden2 = 128\n",
    "\n",
    "inputs = layers.Input(shape=(num_states,))\n",
    "common1 = layers.Dense(num_hidden1, activation=\"relu\")(inputs)\n",
    "common2 = layers.Dense(num_hidden2, activation=\"relu\")(common1)\n",
    "action = layers.Dense(num_actions, activation=\"softmax\")(common2)\n",
    "critic = layers.Dense(1, activation=\"linear\")(common2)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=[action, critic])\n",
    "\n",
    "print('model.summary in get_actor',model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14580b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xapp = None\n",
    "pos = 0\n",
    "cell_data = None\n",
    "rmr_xapp = None\n",
    "\n",
    "class UENotFound(BaseException):\n",
    "    pass\n",
    "class CellNotFound(BaseException):\n",
    "    pass\n",
    "\n",
    "def post_init(self):\n",
    "    print('///////enter def post_init__/////////////////')\n",
    "    \"\"\"\n",
    "    Function that runs when xapp initialization is complete\n",
    "    \"\"\"\n",
    "    self.def_hand_called = 0\n",
    "    self.traffic_steering_requests = 0\n",
    "\n",
    "\n",
    "def handle_config_change(self, config):\n",
    "    print('////////enter def handle_config_change//////////////')\n",
    "    \"\"\"\n",
    "    Function that runs at start and on every configuration file change.\n",
    "    \"\"\"\n",
    "    self.logger.debug(\"handle_config_change: config: {}\".format(config))\n",
    "\n",
    "\n",
    "def default_handler(self, summary, sbuf):\n",
    "    print('/////////enter def default_handler///////////////')\n",
    "    \"\"\"\n",
    "    Function that processes messages for which no handler is defined\n",
    "    \"\"\"\n",
    "    self.def_hand_called += 1\n",
    "    print('self.def_hand_called += 1=', self.def_hand_called)\n",
    "    self.logger.warning(\"default_handler unexpected message type {}\".format(summary[rmr.RMR_MS_MSG_TYPE]))\n",
    "    self.rmr_free(sbuf)\n",
    "\n",
    "\n",
    "def mr_req_handler(self, summary, sbuf):\n",
    "    print('///////////enter def mr_req handler/////////////')\n",
    "    \"\"\"\n",
    "    This is the main handler for this xapp, which handles load prediction requests.\n",
    "    This app fetches a set of data from SDL, and calls the predict method to perform\n",
    "    prediction based on the data\n",
    "\n",
    "    The incoming message that this function handles looks like:\n",
    "        {\"UEPredictionSet\" : [\"UEId1\",\"UEId2\",\"UEId3\"]}\n",
    "    \"\"\"\n",
    "    #self.traffic_steering_requests += 1\n",
    "    # we don't use rts here; free the buffer\n",
    "    self.rmr_free(sbuf)\n",
    "\n",
    "    ue_list = []\n",
    "    try:\n",
    "        print('////enter first try in mr_req_handler////')\n",
    "        print('rmr.RMR_MS_PAYLOAD=', rmr.RMR_MS_PAYLOAD)\n",
    "        print('summary[rmr.RMR_MS_PAYLOAD]=', summary[rmr.RMR_MS_PAYLOAD])\n",
    "        req = json.loads(summary[rmr.RMR_MS_PAYLOAD])  # input should be a json encoded as bytes\n",
    "        print('req = json.loads(summary[rmr.RMR_MS_PAYLOAD])=', req)\n",
    "        ue_list = req[\"UEPredictionSet\"]\n",
    "        print('ue_list=req[\"UEPredictionSet\"] =', ue_list)\n",
    "        self.logger.debug(\"mr_req_handler processing request for UE list {}\".format(ue_list))\n",
    "    except (json.decoder.JSONDecodeError, KeyError):\n",
    "        print('////enter first except in mr_req_handler////')\n",
    "        self.logger.warning(\"mr_req_handler failed to parse request: {}\".format(summary[rmr.RMR_MS_PAYLOAD]))\n",
    "        return\n",
    "    print('ue_list mr_req_handler aftr 1st try=', ue_list)\n",
    "    # iterate over the UEs, fetches data for each UE and perform prediction\n",
    "    for ueid in ue_list:\n",
    "        try:\n",
    "            print('////enter second try in mr_req_handler////')\n",
    "            uedata = sdl.get_uedata(self, ueid)\n",
    "            print('uedata = sdl.get_uedata(self, ueid)=', uedata)\n",
    "            predict(self, uedata)\n",
    "            print('predict(self, uedata)=', predict(self, uedata))\n",
    "        except UENotFound:\n",
    "            print('////enter second except in mr_req_handler////')\n",
    "            print('enter UENotFound in mr_req_handler')\n",
    "            self.logger.warning(\"mr_req_handler received a TS Request for a UE that does not exist!\")\n",
    "\n",
    "def entry():\n",
    "    print('////////////enter def entry///////////////')\n",
    "    \"\"\"  Read from DB in an infinite loop and run prediction every second\n",
    "      TODO: do training as needed in the future\n",
    "    \"\"\"\n",
    "    schedule.every(1).seconds.do(RL)\n",
    "    print('/////////pass 1 entry schedule.every(1).seconds.do(run_prediction, self)/////')\n",
    "    while True:\n",
    "        #print('////while True in entry/////') \n",
    "        schedule.run_pending()\n",
    "\n",
    "        \n",
    "        \n",
    "def RL():\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "    huber_loss = keras.losses.Huber()\n",
    "    action_probs_history = []\n",
    "    actions_probs_history = []\n",
    "    action_probs_history_test = []\n",
    "    critic_value_history = []\n",
    "    critic_value_history_test = []\n",
    "    rewards_history = []\n",
    "    reward_history_for_plot = []\n",
    "    running_rewards_history = []\n",
    "    episode_reward_history = []\n",
    "    running_reward = 0\n",
    "    episode_count = 0\n",
    "    iteration = 0\n",
    "    utility_history = []\n",
    "    episode_utility = 0\n",
    "    episode_utility_history = []\n",
    "    mean_utility_history = []\n",
    "    actor_loss_history = []\n",
    "    critic_loss_history = []\n",
    "    critic_loss_history_test = []\n",
    "    loss_value_history_whole = []\n",
    "\n",
    "    while True:  # Run until solved\n",
    "        state = env.reset()\n",
    "        print('tensor state in while True=', state)\n",
    "        episode_reward = 0\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            for timestep in range(1, max_steps_per_episode):\n",
    "                print('timestep=', timestep)\n",
    "                # env.render(); Adding this line would show the attempts\n",
    "                # of the agent in a pop up window.\n",
    "                prev_state = state\n",
    "                #print('prev_state in main=', prev_state)\n",
    "\n",
    "                state_per_user = [0]*num_states\n",
    "                #state_per_user = []\n",
    "\n",
    "                for index in range (num_ues):\n",
    "\n",
    "                    state_per_user[index] = state[(index*num_states):((index*num_states)+num_states)]\n",
    "                    #state_per_user.append(state[(index*num_states):((index*num_states)+num_states)])\n",
    "                    #print('state_per_user[index]=', state_per_user[index])\n",
    "\n",
    "\n",
    "                state_all_users = []\n",
    "                for i in range (num_ues):\n",
    "                    state_all_users.append(state_per_user[:][i])\n",
    "                #print('state_all_users=', state_all_users)\n",
    "                state_all_users = tf.convert_to_tensor(state_all_users)    \n",
    "                #print('tf.convert_to_tensor(state_all_users)=', state_all_users)\n",
    "\n",
    "                action_probs, critic_value = model(state_all_users)\n",
    "                #print('action_probs=', action_probs)\n",
    "                #print('critic_value=', critic_value)\n",
    "                #print('critic_value[:, 0]=', critic_value[:, 0])\n",
    "                critic_value_history.append(critic_value[:, 0])\n",
    "                #print('critic_value_history.append(critic_value[0, 0])', critic_value_history)\n",
    "                critic_value_history_test.append(critic_value)\n",
    "                #print('critic_value_history_test.append(critic_value)=', critic_value_history_test)\n",
    "\n",
    "\n",
    "\n",
    "                # Sample action from action probability distribution\n",
    "                #print('paaaaaaaaaaasssseeeddddddddddd shittttttttttttttt')\n",
    "                actions = [0]*num_ues\n",
    "                action_probs_per_action = []\n",
    "                p_whole = []\n",
    "                for i in range(num_ues):\n",
    "                    #print('i=', i)\n",
    "                    #print('action_probs[i]=', action_probs[i])\n",
    "                    #print('action_probs[i,:]=', action_probs[i,:])\n",
    "                    #print('p=np.squeeze(action_probs[i,:])=', np.squeeze(action_probs[i,:]))\n",
    "                    #print('tf.squeeze(action_probs[i,:])=', tf.squeeze(action_probs[i,:]))\n",
    "                    action = np.random.choice(num_actions, p=np.squeeze(action_probs[i]))\n",
    "\n",
    "                    #print('action=', action)\n",
    "                    actions[i]=action\n",
    "                    #print('actions=', actions)\n",
    "                    action_probs_per_action.append(action_probs[i, action])\n",
    "                    #print('action_probs_per_action.append(action_probs[i, action])=', action_probs_per_action)\n",
    "\n",
    "                    #print('tf.math.log(action_probs[i, action])=', tf.math.log(action_probs[i, action]))\n",
    "                    action_probs_history.append(tf.math.log(action_probs[i, action]))\n",
    "                    #print('action_probs_history.append(tf.math.log(action_probs[i, action]))=', action_probs_history)\n",
    "\n",
    "                #print('tf.math.log(action_probs_per_action)=', tf.math.log(action_probs_per_action))\n",
    "                action_probs_history_test.append(tf.math.log(action_probs_per_action))\n",
    "                #print('action_probs_history_test.append(tf.math.log(action_probs_per_action))=',action_probs_history_test)\n",
    "                actions_probs_history.append(action_probs_history)\n",
    "                #print('actions_probs_history=', actions_probs_history)\n",
    "                actions_tensor = tf.convert_to_tensor(actions) \n",
    "                print('actions_tensor=', actions_tensor)\n",
    "                actions = np.asarray(actions, dtype=np.int64)\n",
    "                #print('actions_a_array=', actions_a)\n",
    "\n",
    "\n",
    "                # Apply the sampled action in our environment\n",
    "                \n",
    "                state, reward, done = connectdb(actions)\n",
    "                #state, reward, done, info = env.step(actions)\n",
    "                #network_reward = self.utilities_scaled_float_mean\n",
    "                #print('state=state, reward, done = connectdb(actions)=', state)\n",
    "                state = np.array(state, dtype='float32')\n",
    "                #print('state = np.array(state)=', state)\n",
    "                print('reward:connectdb(actions)=', reward)\n",
    "                #print('network_reward=', network_reward)\n",
    "                print('done=env.step(actions)=', done)\n",
    "                #print('info=env.step(actions)=', info)\n",
    "                #print('utility=env.step(actions)=', utility)\n",
    "                rewards_history.append(reward)\n",
    "                reward_history_for_plot.append(reward)\n",
    "                #utility_history.append(utility)\n",
    "                #print('rewards_history.append(reward)=', rewards_history)\n",
    "                episode_reward += reward\n",
    "                #print('episode_reward += reward=', episode_reward)\n",
    "                episode_reward_history.append(episode_reward)\n",
    "                #episode_utility +=utility\n",
    "                #episode_utility_history.append(episode_utility)\n",
    "\n",
    "\n",
    "                if done:\n",
    "\n",
    "                    break\n",
    "\n",
    "            iteration +=1\n",
    "            # load all tracked results as pandas data frames\n",
    "            scalar_results_1 = env.monitor.load_results()\n",
    "\n",
    "            # show general specific results\n",
    "            #scalar_results_1.head()\n",
    "\n",
    "            mean_utility_history.append(scalar_results_1['mean utility'].tolist())\n",
    "\n",
    "            # Update running reward to check condition for solving\n",
    "\n",
    "            running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "            #print('running_reward = 0.05 * epi=', running_reward)\n",
    "            running_rewards_history.append(running_reward)\n",
    "            # Calculate expected value from rewards\n",
    "            # - At each timestep what was the total reward received after that timestep\n",
    "            # - Rewards in the past are discounted by multiplying them with gamma\n",
    "            # - These are the labels for our critic\n",
    "            returns = []\n",
    "            discounted_sum = 0\n",
    "            for r in rewards_history[::-1]:\n",
    "                discounted_sum = r + gamma * discounted_sum\n",
    "                returns.insert(0, discounted_sum)\n",
    "            #print('returns=', returns)\n",
    "\n",
    "            # Normalize\n",
    "            returns = np.array(returns)\n",
    "            returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
    "            #print('returns=(returns - np.mean(returns)) / (np.st', returns)\n",
    "            returns = returns.tolist()\n",
    "            #print('returns.tolist()=', returns)\n",
    "\n",
    "            # Calculating loss values to update our network\n",
    "            history = zip(action_probs_history_test, critic_value_history, returns, critic_value_history_test)\n",
    "            #print('history = zip(action_probs_history, critic_value_history, returns)=', history)\n",
    "            actor_losses = []\n",
    "            critic_losses = []\n",
    "            critic_losses_test = []\n",
    "            for log_prob, value, ret, value_test in history:\n",
    "\n",
    "\n",
    "\n",
    "                #print('log_prob=', log_prob)\n",
    "                #print('value=', value)\n",
    "                #print('ret=', ret)\n",
    "                diff = ret - value\n",
    "                #print('diff in for loop of history=', diff)\n",
    "\n",
    "                #print('diff.mul(log_prob)=', tf.multiply(-log_prob,diff))\n",
    "                #print('-log_prob * diff=', -log_prob*diff)\n",
    "                actor_losses.append(-log_prob*diff)  # actor loss\n",
    "                #actor_losses.append(tf.multiply(-log_prob,diff))  # actor loss\n",
    "                actor_loss_history.append(actor_losses)\n",
    "                #print('actor_losses.append(-log_prob * diff)=', actor_losses)\n",
    "\n",
    "   \n",
    "\n",
    "                #print('value_test=', value_test)\n",
    "                #print('tf.expand_dims(value_test, 0)=', tf.expand_dims(value, 0))\n",
    "                #print('huber_loss(tf.expand_dims(value_test, 0), tf.expand_dims(ret, 0))=', huber_loss(value_test, tf.expand_dims(ret, 0)))\n",
    "                critic_losses.append(\n",
    "                    huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
    "                )\n",
    "                critic_loss_history.append(critic_losses)\n",
    "                #print('critic_losses=', critic_losses)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # Backpropagation\n",
    "            loss_value_history = []\n",
    "            grads_history = []\n",
    "            #print('actor_losses=', actor_losses)\n",
    "            #print('critic_losses=', critic_losses)\n",
    "            #print('sum(actor_losses)=', sum(actor_losses))\n",
    "            #print('sum(critic_losses)=', sum(critic_losses))\n",
    "            loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "            #print('loss_value = sum(actor_losses) + sum(critic_losses)=', loss_value)\n",
    "            loss_value_history.append(loss_value)\n",
    "            loss_value_history_whole.append(loss_value)\n",
    "            #print('loss_value for backprpagation=', loss_value)\n",
    "            grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "            #print('grads = tape.gradient(loss_value, model.trainable_variables)=', grads)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            grads_history.append(grads)       \n",
    "            #print('iteration=', iteration)\n",
    "\n",
    "            # Clear the loss and reward history\n",
    "            action_probs_history.clear()\n",
    "            critic_value_history.clear()\n",
    "            rewards_history.clear()\n",
    "\n",
    "        # Log details\n",
    "        episode_count += 1\n",
    "        print('episode_count += 1=', episode_count)\n",
    "        if episode_count % 10 == 0:\n",
    "            template = \"running reward: {:.2f} at episode {}\"\n",
    "            print(template.format(running_reward, episode_count))\n",
    "\n",
    "        scalar_results_2 = env.monitor.load_results()\n",
    "    #     if scalar_results_2['mean datarate'].mean() > 20 and scalar_results_2['mean datarate'].max() < 130:\n",
    "\n",
    "\n",
    "\n",
    "        if episode_count > 1:  # Condition to consider the task solved\n",
    "            print(\"Solved at episode {}!\".format(episode_count))\n",
    "\n",
    "            scalar_results_2 = env.monitor.load_results()\n",
    "\n",
    "            plt.plot(reward_history_for_plot)\n",
    "            plt.xlabel(\"episode_count\")\n",
    "            plt.ylabel(\"reward_history_for_plot\")\n",
    "            plt.show()    \n",
    "\n",
    "            plt.plot(running_rewards_history)\n",
    "            plt.xlabel(\"episode_count\")\n",
    "            plt.ylabel(\"running_rewards_history\")\n",
    "            plt.show()\n",
    "\n",
    "            plt.plot(episode_reward_history)\n",
    "            plt.xlabel(\"episode_count\")\n",
    "            plt.ylabel(\"episode_reward\")\n",
    "            plt.show()\n",
    "\n",
    "            plt.plot(loss_value_history)\n",
    "            plt.xlabel(\"iteration\")\n",
    "            plt.ylabel(\"loss_value_history\")\n",
    "            plt.show()\n",
    "\n",
    "            plt.plot(utility_history)\n",
    "            plt.xlabel(\"episode_count\")\n",
    "            plt.ylabel(\"utility_history\")\n",
    "            plt.show()\n",
    "\n",
    "            plt.plot(episode_utility_history)\n",
    "            plt.xlabel(\"episode_count\")\n",
    "            plt.ylabel(\"episode_utility_history\")\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "            break\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def connectdb(action):\n",
    "    print('////////////////////enter def connectdb///////////////////')\n",
    "    # Create a connection to InfluxDB if thread=True, otherwise it will create a dummy data instance\n",
    "    global db\n",
    "    global cell_data\n",
    "    \n",
    "    print('//////enter else= populate.populate()////////////////')  \n",
    "    populate.populatedb(action)  # temporary method to populate db, it will be removed when data will be coming through KPIMON to influxDB\n",
    "\n",
    "    print('////came back from populate to connectdb.else:, db=DATABASE(CellData)///////')\n",
    "    db = DATABASE('CellData')\n",
    "    print('////came back from db.DATABASE-init to connectdb.else///////')\n",
    "    print('db =  DATABASE(celldata) =', db) \n",
    "    db.read_data(\"cellMeas\")\n",
    "    print('////came back from db.DATABASE-read-data to connectdb.else///////')\n",
    "    print('db.read_data(\"cellMeas\")=', db.read_data(\"cellMeas\"))\n",
    "    cell_data = db.data.values.tolist()  # needs to be updated in future when live feed will be coming through KPIMON to influxDB\n",
    "    print('cell_data = db.data.values.tolist()=', cell_data)\n",
    "    obs = [0] * 35\n",
    "    #print('obs=', obs)\n",
    "    obs[0:2] = cell_data[0][0:2]\n",
    "    #print('obs=', obs)\n",
    "    obs[2] = cell_data[0][12]\n",
    "    #print('obs=', obs)\n",
    "    obs[3] = cell_data[0][23]\n",
    "    #print('obs=', obs)\n",
    "    obs[4:10] = cell_data[0][31:37]\n",
    "    #print('obs=', obs)\n",
    "    obs[10:20] = cell_data[0][2:12]\n",
    "    #print('obs=', obs)\n",
    "    obs[20:29] = cell_data[0][13:23]\n",
    "    #print('obs=', obs)\n",
    "    obs[29:35] = cell_data[0][24:29]\n",
    "    print('obs=', obs)\n",
    "\n",
    "    reward = cell_data[0][29]\n",
    "    done = cell_data[0][30]\n",
    "\n",
    "    #print('cell_data:, cell_data)\n",
    "    print('///////connectdb finished go to start//////')\n",
    "    return obs, reward, done\n",
    " \n",
    "\n",
    "\n",
    "def start(thread=False):\n",
    " \n",
    "    print('////////////////entered Starrrrrrrrrrrt///////////////////')\n",
    "    \"\"\"\n",
    "    This is a convenience function that allows this xapp to run in Docker\n",
    "    for \"real\" (no thread, real SDL), but also easily modified for unit testing\n",
    "    (e.g., use_fake_sdl). The defaults for this function are for the Dockerized xapp.\n",
    "    \"\"\"\n",
    "    global xapp\n",
    "\n",
    "    #fake_sdl = getenv(\"USE_FAKE_SDL\", None)\n",
    "    #xapp = Xapp(entrypoint=entry, rmr_port=4560, use_fake_sdl=False)\n",
    "    #print('xapp = Xapp(entrypoint=entry, rmr_port=4560, use_fake_sdl=fake_sdl)=', xapp)\n",
    "  \n",
    "    use_fake_sdl=False\n",
    "    rmr_port=4560\n",
    "    entry()\n",
    "    \n",
    "    #xapp.run()\n",
    "\n",
    "\n",
    "def stop():\n",
    "    print('/////////////enter def stop//////////////////')      \n",
    "    \"\"\"\n",
    "    can only be called if thread=True when started\n",
    "    \"\"\"\n",
    "    xapp.stop()\n",
    "\n",
    "\n",
    "def get_stats():\n",
    "    print('//////////////////enter def get_stats()////////////////////')\n",
    "    \"\"\"\n",
    "    hacky for now, will evolve\n",
    "    \"\"\"\n",
    "    print('DefCalled:rmr_xapp.def_hand_called=', rmr_xapp.def_hand_called)\n",
    "    print('SteeringRequests:rmr_xapp.traffic_steering_requests=', rmr_xapp.traffic_steering_requests) \n",
    "    return {\"DefCalled\": rmr_xapp.def_hand_called,\n",
    "            \"SteeringRequests\": rmr_xapp.traffic_steering_requests}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe44928",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('////////////////entered Starrrrrrrrrrrt///////////////////')\n",
    "\"\"\"\n",
    "This is a convenience function that allows this xapp to run in Docker\n",
    "for \"real\" (no thread, real SDL), but also easily modified for unit testing\n",
    "(e.g., use_fake_sdl). The defaults for this function are for the Dockerized xapp.\n",
    "\"\"\"\n",
    "thread = False\n",
    "global xapp\n",
    "#fake_sdl = getenv(\"USE_FAKE_SDL\", None)\n",
    "#xapp = Xapp(entrypoint=entry, rmr_port=4560, use_fake_sdl=False)\n",
    "\n",
    "#obs, reward, done = connectdb(thread)\n",
    "#print('obs after connectdb =', obs)\n",
    "#print('reward after connectdb =', reward)\n",
    "#print('done after connectdb =', done)\n",
    "#print('///////come back from connectdb////////')\n",
    "\n",
    "\n",
    "use_fake_sdl=False\n",
    "rmr_port=4560\n",
    "#xapp= entry(self)\n",
    "#xapp.run()\n",
    "entry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5dc724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bff6ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
